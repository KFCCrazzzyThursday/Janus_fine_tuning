{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ddp_setup():\n",
    "    \"\"\"\n",
    "    初始化分布式进程组，并设置当前进程使用哪张GPU。\n",
    "    LOCAL_RANK/ RANK / WORLD_SIZE 都是 torchrun 自动传入的环境变量\n",
    "    \"\"\"\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])  # 当前进程在本机的 GPU 编号\n",
    "    torch.cuda.set_device(local_rank)           # 让本进程只用 local_rank 对应的 GPU\n",
    "    return local_rank\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_images(\n",
    "    mmgpt: MultiModalityCausalLM,\n",
    "    vl_chat_processor: VLChatProcessor,\n",
    "    prompt: str,\n",
    "    temperature: float = 1.0,\n",
    "    parallel_size: int = 2,\n",
    "    cfg_weight: float = 5.0,\n",
    "    image_token_num_per_image: int = 576,\n",
    "    img_size: int = 384,\n",
    "    patch_size: int = 16,\n",
    "):\n",
    "    \"\"\"\n",
    "    与之前的单卡逻辑类似，但要注意我们在外部已经把模型包装成 DDP，\n",
    "    因此如果 mmgpt 是 DDP 对象，需要访问底层模型 (mmgpt.module)。\n",
    "    另外我们可以在 batch 维度并行，从而让多卡分摊计算。\n",
    "    \"\"\"\n",
    "    is_ddp = isinstance(mmgpt, DDP)\n",
    "    if is_ddp:\n",
    "        real_model = mmgpt.module\n",
    "    else:\n",
    "        real_model = mmgpt\n",
    "\n",
    "    # 把 prompt 编码成 tokens\n",
    "    input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "    input_ids = torch.LongTensor(input_ids).cuda()\n",
    "\n",
    "    # 构建一次性 batch = parallel_size*2，用于 CFG（cond/uncond）\n",
    "    tokens = torch.zeros((parallel_size * 2, len(input_ids)), dtype=torch.int).cuda()\n",
    "    for i in range(parallel_size * 2):\n",
    "        tokens[i, :] = input_ids\n",
    "        if i % 2 != 0:\n",
    "            tokens[i, 1:-1] = vl_chat_processor.pad_id\n",
    "\n",
    "    # 得到输入的 embeddings\n",
    "    inputs_embeds = real_model.language_model.get_input_embeddings()(tokens)\n",
    "    # 准备存放生成的图像tokens\n",
    "    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n",
    "\n",
    "    past_key_values = None\n",
    "    for step_i in range(image_token_num_per_image):\n",
    "        # 喂给底层语言模型\n",
    "        outputs = real_model.language_model.model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # 执行 CFG\n",
    "        logits = real_model.gen_head(hidden_states[:, -1, :])\n",
    "        logits_cond = logits[0::2, :]\n",
    "        logits_uncond = logits[1::2, :]\n",
    "        logits = logits_uncond + cfg_weight * (logits_cond - logits_uncond)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # [parallel_size, 1]\n",
    "        generated_tokens[:, step_i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "        # 把 cond / uncond 拼回\n",
    "        next_token = torch.cat([next_token, next_token], dim=1).view(-1)\n",
    "\n",
    "        # 准备下一步图像embedding\n",
    "        img_embeds = real_model.prepare_gen_img_embeds(next_token)\n",
    "        inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "    # 解码为图像\n",
    "    dec = real_model.gen_vision_model.decode_code(\n",
    "        generated_tokens.to(dtype=torch.int),\n",
    "        shape=[parallel_size, 8, img_size // patch_size, img_size // patch_size]\n",
    "    )\n",
    "    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n",
    "\n",
    "    # 转成可保存格式\n",
    "    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n",
    "    visual_img[:, :, :] = dec\n",
    "\n",
    "    os.makedirs('generated_samples', exist_ok=True)\n",
    "    # 给每张图像加 step_i 后缀或者自己想要的名字\n",
    "    for i in range(parallel_size):\n",
    "        save_path = os.path.join('generated_samples', f\"rank{dist.get_rank()}_img_{i}.jpg\")\n",
    "        PIL.Image.fromarray(visual_img[i]).save(save_path)\n",
    "\n",
    "    print(f\"Rank{dist.get_rank()} 生成图像完成并已保存到 generated_samples/ 下。\")\n",
    "\n",
    "def main():\n",
    "    local_rank = ddp_setup()\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # （1）加载模型和处理器\n",
    "    # ----------------------------------------------------------\n",
    "    model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "    cache_dir  = \"/workspace/liyj/ckpt/JanusPro7B\"\n",
    "\n",
    "    print(f\"[Rank {dist.get_rank()}] 正在加载处理器和模型...\")\n",
    "\n",
    "    vl_chat_processor = VLChatProcessor.from_pretrained(model_path, cache_dir=cache_dir)\n",
    "    vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, trust_remote_code=True, cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    # 转到本进程使用的那张 GPU\n",
    "    vl_gpt = vl_gpt.to(torch.bfloat16).cuda(local_rank)\n",
    "    vl_gpt.eval()\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # （2）构建 DDP 包装\n",
    "    # ----------------------------------------------------------\n",
    "    vl_gpt = DDP(vl_gpt, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # （3）准备对话 / prompt\n",
    "    # ----------------------------------------------------------\n",
    "    question = '''\n",
    "    \"question\":\"Which of the following could Gordon's test show?\",\n",
    "        \"choices\":[\n",
    "          \"if the spacecraft was damaged when using a parachute with a 1 m vent going 200 km per hour\",\n",
    "          \"how steady a parachute with a 1 m vent was at 200 km per hour\",\n",
    "          \"whether a parachute with a 1 m vent would swing too much at 400 km per hour\"\n",
    "        ],\n",
    "        \"answer\":1,\n",
    "        \"hint\":\"People can use the engineering-design process...\",\n",
    "        \"skill\":\"Evaluate tests of engineering-design solutions\",\n",
    "        \"lecture\":\"People can use the engineering-design process...\"\n",
    "    '''\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"<|User|>\",\n",
    "            \"content\": \"Generate an image compatible for this QA pair:\",\n",
    "        },\n",
    "        {\"role\": \"<|Assistant|>\", \"content\": f\"{question}\"},\n",
    "    ]\n",
    "\n",
    "    # 应用 SFT 模板\n",
    "    sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "        conversations=conversation,\n",
    "        sft_format=vl_chat_processor.sft_format,\n",
    "        system_prompt=\"\"\n",
    "    )\n",
    "    prompt = sft_format + vl_chat_processor.image_start_tag\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # （4）调用生成函数\n",
    "    #     这里设置 parallel_size=2 以便有batch维度让两张卡都能并行\n",
    "    # ----------------------------------------------------------\n",
    "    print(f\"[Rank {dist.get_rank()}] 开始生成图像...\")\n",
    "    generate_images(\n",
    "        mmgpt=vl_gpt,\n",
    "        vl_chat_processor=vl_chat_processor,\n",
    "        prompt=prompt,\n",
    "        temperature=1.0,\n",
    "        parallel_size=2,   # 若显存不足，可调得更小；若想批量更多，可调大\n",
    "        cfg_weight=5.0,\n",
    "        image_token_num_per_image=576,\n",
    "        img_size=384,\n",
    "        patch_size=16,\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # （5）销毁进程组，结束\n",
    "    # ----------------------------------------------------------\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 如果你使用 torchrun --nproc_per_node=2 ddp_inference.py 启动，此处会并行启动2个进程\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
